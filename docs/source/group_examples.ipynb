{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of using the `GroupOperation` class - Manipulating a Group\n",
    "\n",
    "Any operation to be applied to a specific group can be accessed via the ``GroupOperation`` class, imported as below from padocc's ``operations`` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PADOCC Group: my_group>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from padocc.operations import GroupOperation\n",
    "import logging\n",
    "\n",
    "my_group = GroupOperation(\n",
    "    'my_group',\n",
    "    workdir='../../auto_testdata_dir', # The directory to create pipeline files.\n",
    "    verbose=logging.INFO,\n",
    ")\n",
    "my_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Get some general information about the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: my_group\n",
      "General Methods:\n",
      " > group.run() - Run a specific operation across part of the group.\n",
      " > group.init_from_file() - Initialise the group based on an input csv file\n",
      " > group.init_from_stac() - Initialise the group based on a STAC index\n",
      " > group.add_project() - Add an new project/dataset to this group\n",
      " > group.save_files() - Save any changes to any files in the group as part of an operation\n",
      " > group.check_writable() - Check if all directories are writable for this group.\n",
      "Assessment methods:\n",
      " > group.summary_data() - Get a printout summary of data representations in this group\n",
      " > group.remove_projects() - Remove projects fitting some parameters from this group\n",
      " > group.progress_display() - Get a human-readable display of progress within the group.\n",
      " > group.progress_repr() - Get a dict version of the progress report (for AirFlow)\n"
     ]
    }
   ],
   "source": [
    "my_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: my_group\n",
      " - Workdir: ../../auto_testdata_dir\n",
      " - Groupdir: ../../auto_testdata_dir/groups/my_group\n",
      " - forceful: False\n",
      " - thorough: False\n",
      " - dryrun: False\n"
     ]
    }
   ],
   "source": [
    "my_group.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the group from a file\n",
    "The group has been created but contains no data currently, so we need to fill it from either a file or STAC index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO [group-operation]: Starting initialisation\n",
      "INFO [group-operation]: Copying input file from relative path - resolved to /home/users/dwest77/cedadev/padocc/docs/source\n",
      "INFO [group-operation]: Creating project directories\n",
      "INFO [group-operation]: Creating directories/filelists for 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING [group-operation]: \"../../auto_testdata_dir/in_progress/my_group/padocc-test-1\" already exists.\n",
      "WARNING [group-operation]: \"../../auto_testdata_dir/in_progress/my_group/padocc-test-1/phase_logs\" already exists.\n",
      "INFO [group-operation]: Updated new status: init - Success\n",
      "INFO [group-operation]: Creating directories/filelists for 2/2\n",
      "WARNING [group-operation]: \"../../auto_testdata_dir/in_progress/my_group/padocc-test-2\" already exists.\n",
      "WARNING [group-operation]: \"../../auto_testdata_dir/in_progress/my_group/padocc-test-2/phase_logs\" already exists.\n",
      "INFO [group-operation]: Updated new status: init - Success\n",
      "INFO [group-operation]: Created 12 files, 4 directories in group my_group\n",
      "INFO [group-operation]: Written as group ID: my_group\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../../tests/data/myfile.csv'\n",
    "my_group.init_from_file(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group has now been initialised. The CSV file we loaded contains two 'projects' which will each produce a single dataset object at the end of the pipeline. This is an aggregation of multiple data files into a single product. We can view the contents of the CSV file the group was loaded with as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padocc-test-1,/home/users/dwest77/cedadev/padocc/tests/data/test1.txt,,\n",
      "padocc-test-2,/home/users/dwest77/cedadev/padocc/tests/data/test2.txt,,\n"
     ]
    }
   ],
   "source": [
    "print(my_group.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each project came with a file listing all the data files under that project, in this case there are 5 netCDF files in each project, which we can find using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/dwest77/cedadev/padocc/tests/data/rain/example1.0.nc\n",
      "/home/users/dwest77/cedadev/padocc/tests/data/rain/example1.1.nc\n",
      "/home/users/dwest77/cedadev/padocc/tests/data/rain/example1.2.nc\n",
      "/home/users/dwest77/cedadev/padocc/tests/data/rain/example1.3.nc\n",
      "/home/users/dwest77/cedadev/padocc/tests/data/rain/example1.4.nc\n"
     ]
    }
   ],
   "source": [
    "project = my_group.get_project('padocc-test-1')\n",
    "print(project.allfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a group operation\n",
    "\n",
    "We can now run a process on the group as a whole via the ``run`` method. There are three main phases that form the central section of the pipeline; ``scan``, ``compute`` and ``validate``. These can be run individually (recommended) or if you are running for a single project you may run all steps with ``all``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO [group-operation]: Starting operation: 1/2 (padocc-test-1)\n",
      "INFO [project-operation_0]: Starting scan-kerchunk operation for padocc-test-1\n",
      "INFO [project-operation_0]: Starting scan-kerchunk operation for padocc-test-1\n",
      "INFO [project-operation_0]: Determined 2 files to scan (out of 5)\n",
      "INFO [project-operation_0]: Determined 2 files to scan (out of 5)\n",
      "INFO [project-operation_0]: Starting scan process for Kerchunk cloud format\n",
      "INFO [project-operation_0]: Starting scan process for Kerchunk cloud format\n",
      "INFO [project-operation_0]: Starting computation for components of padocc-test-1\n",
      "INFO [project-operation_0]: Starting computation for components of padocc-test-1\n",
      "INFO [project-operation_0]: Loading cache file\n",
      "INFO [project-operation_0]: Loading cache file\n",
      "INFO [project-operation_0]: Loaded refs: 1/2\n",
      "INFO [project-operation_0]: Loaded refs: 1/2\n",
      "INFO [project-operation_0]: Loading cache file\n",
      "INFO [project-operation_0]: Loading cache file\n",
      "INFO [project-operation_0]: Loaded refs: 2/2\n",
      "INFO [project-operation_0]: Loaded refs: 2/2\n",
      "INFO [project-operation_0]: Starting concatenation of refs\n",
      "INFO [project-operation_0]: Starting concatenation of refs\n",
      "/home/users/dwest77/cedadev/padocc/.local/lib/python3.11/site-packages/xarray/backends/zarr.py:1097: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  ds = open_dataset(\n",
      "/home/users/dwest77/cedadev/padocc/.local/lib/python3.11/site-packages/xarray/backends/zarr.py:1097: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  ds = open_dataset(\n",
      "INFO [project-operation_0]: Determining concatenation dimensions\n",
      "INFO [project-operation_0]: Determining concatenation dimensions\n",
      "INFO [project-operation_0]: Found ['time'] concatenation dimensions.\n",
      "INFO [project-operation_0]: Found ['time'] concatenation dimensions.\n",
      "INFO [project-operation_0]: Determining identical variables\n",
      "INFO [project-operation_0]: Determining identical variables\n",
      "INFO [project-operation_0]: Found ['latitude', 'longitude'] identical variables.\n",
      "INFO [project-operation_0]: Found ['latitude', 'longitude'] identical variables.\n",
      "INFO [project-operation_0]: Concatenating to JSON format Kerchunk file\n",
      "INFO [project-operation_0]: Concatenating to JSON format Kerchunk file\n",
      "INFO [project-operation_0]: Skipped writing to JSON file - None\n",
      "INFO [project-operation_0]: Skipped writing to JSON file - None\n",
      "INFO [project-operation_0]: Details updated in detail-cfg.json\n",
      "INFO [project-operation_0]: Details updated in detail-cfg.json\n",
      "INFO [project-operation_0]: Summarising scan results for 2 files\n",
      "INFO [project-operation_0]: Summarising scan results for 2 files\n",
      "INFO [project-operation_0]: Data recorded for file 1\n",
      "INFO [project-operation_0]: Data recorded for file 1\n",
      "INFO [project-operation_0]: Data recorded for file 2\n",
      "INFO [project-operation_0]: Data recorded for file 2\n",
      "INFO [project-operation_0]: Summary complete, compiling outputs\n",
      "INFO [project-operation_0]: Summary complete, compiling outputs\n",
      "INFO [project-operation_0]: Updated new status: scan - Success\n",
      "INFO [project-operation_0]: Updated new status: scan - Success\n",
      "INFO [group-operation]: Starting operation: 2/2 (padocc-test-2)\n",
      "INFO [project-operation_1]: Starting scan-kerchunk operation for padocc-test-2\n",
      "INFO [project-operation_1]: Determined 2 files to scan (out of 5)\n",
      "INFO [project-operation_1]: Starting scan process for Kerchunk cloud format\n",
      "INFO [project-operation_1]: Starting computation for components of padocc-test-2\n",
      "INFO [project-operation_1]: Loading cache file\n",
      "INFO [project-operation_1]: Loaded refs: 1/2\n",
      "INFO [project-operation_1]: Loading cache file\n",
      "INFO [project-operation_1]: Loaded refs: 2/2\n",
      "INFO [project-operation_1]: Starting concatenation of refs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/dwest77/cedadev/padocc/.local/lib/python3.11/site-packages/xarray/backends/zarr.py:1097: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  ds = open_dataset(\n",
      "/home/users/dwest77/cedadev/padocc/.local/lib/python3.11/site-packages/xarray/backends/zarr.py:1097: RuntimeWarning: Failed to open Zarr store with consolidated metadata, but successfully read with non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  ds = open_dataset(\n",
      "INFO [project-operation_1]: Determining concatenation dimensions\n",
      "INFO [project-operation_1]: Found ['time'] concatenation dimensions.\n",
      "INFO [project-operation_1]: Determining identical variables\n",
      "INFO [project-operation_1]: Found ['latitude', 'longitude'] identical variables.\n",
      "INFO [project-operation_1]: Concatenating to JSON format Kerchunk file\n",
      "INFO [project-operation_1]: Skipped writing to JSON file - None\n",
      "INFO [project-operation_1]: Details updated in detail-cfg.json\n",
      "INFO [project-operation_1]: Summarising scan results for 2 files\n",
      "INFO [project-operation_1]: Data recorded for file 1\n",
      "INFO [project-operation_1]: Data recorded for file 2\n",
      "INFO [project-operation_1]: Summary complete, compiling outputs\n",
      "INFO [project-operation_1]: Updated new status: scan - Success\n",
      "INFO [group-operation]: Pipeline execution finished\n",
      "INFO [group-operation]: Success: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_group.run(\n",
    "    'scan', \n",
    "    mode='kerchunk', # Default format\n",
    "    repeat_id='main', # All projects\n",
    "    proj_code=None,   # Or run a specific project.\n",
    "    forceful=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to scan part of the existing data to assess viability for the chosen mode of aggregation. In this example, two of the files for each project were converted to kerchunk and combined to ensure the whole dataset can be converted. A detailed file with scan results has been created, which we can access using the same project object as we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition: 0.064 %\n",
      "chunks_per_file: '4.0'\n",
      "driver: hdf5\n",
      "estm_chunksize: 260.28 KB\n",
      "estm_spatial_res: 254.56 deg\n",
      "kerchunk_data: 3.34 KB\n",
      "netcdf_data: 5.21 MB\n",
      "num_files: 5\n",
      "timings:\n",
      "  concat_actual: null\n",
      "  concat_estm: 0.014077\n",
      "  convert_actual: null\n",
      "  convert_estm: 0.014628\n",
      "  validate_actual: null\n",
      "  validate_estm: 0.00494\n",
      "total_chunks: '20.00'\n",
      "type: JSON\n",
      "variable_count: 4\n",
      "variables:\n",
      "- latitude\n",
      "- longitude\n",
      "- p\n",
      "- time\n",
      "version_no: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project = my_group.get_project('padocc-test-1')\n",
    "print(project.detail_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant amount of information present here. The inportant elements are the estimates for the size of the kerchunk file which will be created, the ``type`` which can be ``JSON`` or ``PARQ`` for kerchunk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
